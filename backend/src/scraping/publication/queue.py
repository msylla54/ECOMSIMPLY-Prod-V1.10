"""
Queue et Rate Limiting - Gestion des files de publication et limitation dÃ©bit par boutique - ECOMSIMPLY
"""

import asyncio
import time
from collections import defaultdict, deque
from typing import Dict, List, Optional, Any, Callable
from datetime import datetime, timedelta
import logging

from .dto import PublishTask, PublishBatch, PublicationConfig, PublicationStatus

logger = logging.getLogger(__name__)


class TokenBucket:
    """Token bucket pour rate limiting par store"""
    
    def __init__(self, capacity: int, refill_rate: float):
        """
        Args:
            capacity: Nombre max de tokens
            refill_rate: Tokens ajoutÃ©s par seconde
        """
        self.capacity = capacity
        self.tokens = capacity
        self.refill_rate = refill_rate
        self.last_refill = time.time()
    
    def consume(self, tokens: int = 1) -> bool:
        """
        Consomme des tokens si disponibles
        
        Returns:
            True si tokens consommÃ©s, False sinon
        """
        self._refill()
        
        if self.tokens >= tokens:
            self.tokens -= tokens
            return True
        return False
    
    def _refill(self):
        """Recharge tokens selon le taux"""
        now = time.time()
        elapsed = now - self.last_refill
        tokens_to_add = elapsed * self.refill_rate
        
        self.tokens = min(self.capacity, self.tokens + tokens_to_add)
        self.last_refill = now
    
    def time_until_available(self, tokens: int = 1) -> float:
        """Temps d'attente pour avoir les tokens (secondes)"""
        self._refill()
        
        if self.tokens >= tokens:
            return 0.0
        
        needed = tokens - self.tokens
        return needed / self.refill_rate
    
    def get_stats(self) -> Dict[str, float]:
        """Statistiques du bucket"""
        self._refill()
        return {
            'available_tokens': self.tokens,
            'capacity': self.capacity,
            'fill_percentage': (self.tokens / self.capacity) * 100,
            'refill_rate_per_sec': self.refill_rate
        }


class PublishQueue:
    """Queue de publication avec prioritÃ© et rate limiting"""
    
    def __init__(self, config: PublicationConfig):
        self.config = config
        
        # Queue principale avec prioritÃ©
        self._queue: List[PublishTask] = []
        self._queue_lock = asyncio.Lock()
        
        # Rate limiters par store (Token Bucket)
        self._rate_limiters: Dict[str, TokenBucket] = {}
        
        # Historique pour cooldown par store
        self._last_publication: Dict[str, datetime] = {}
        
        # Statistiques
        self.stats = {
            'tasks_added': 0,
            'tasks_processed': 0,
            'rate_limited_count': 0,
            'queue_depth_max': 0
        }
    
    async def add_task(self, task: PublishTask) -> None:
        """Ajouter tÃ¢che Ã  la queue"""
        
        async with self._queue_lock:
            self._queue.append(task)
            # Trier par prioritÃ© (1 = urgent, 10 = bas)
            self._queue.sort(key=lambda t: (t.priority, t.created_at))
            
            current_depth = len(self._queue)
            self.stats['queue_depth_max'] = max(self.stats['queue_depth_max'], current_depth)
            self.stats['tasks_added'] += 1
            
            logger.debug(f"TÃ¢che ajoutÃ©e Ã  la queue: {task.task_id} (prioritÃ© {task.priority}), "
                        f"depth: {current_depth}")
    
    async def add_batch(self, batch: PublishBatch) -> None:
        """Ajouter un batch de tÃ¢ches"""
        
        for task in batch.tasks:
            await self.add_task(task)
        
        logger.info(f"Batch ajoutÃ©: {batch.batch_id} avec {len(batch.tasks)} tÃ¢ches")
    
    async def get_next_task(self) -> Optional[PublishTask]:
        """
        RÃ©cupÃ©rer prochaine tÃ¢che disponible selon rate limiting
        
        Returns:
            TÃ¢che prÃªte ou None si rate limited
        """
        
        async with self._queue_lock:
            if not self._queue:
                return None
            
            # Chercher premiÃ¨re tÃ¢che disponible selon rate limits
            for i, task in enumerate(self._queue):
                if await self._can_process_task(task):
                    # Retirer tÃ¢che de la queue
                    task = self._queue.pop(i)
                    self.stats['tasks_processed'] += 1
                    
                    # Marquer comme dÃ©marrÃ©e
                    task.mark_started()
                    
                    # Enregistrer derniÃ¨re publication pour cooldown
                    self._last_publication[task.store_id] = datetime.now()
                    
                    logger.debug(f"TÃ¢che rÃ©cupÃ©rÃ©e: {task.task_id} pour store {task.store_id}")
                    return task
            
            # Aucune tÃ¢che disponible â†’ rate limited
            self.stats['rate_limited_count'] += 1
            return None
    
    async def _can_process_task(self, task: PublishTask) -> bool:
        """VÃ©rifier si tÃ¢che peut Ãªtre traitÃ©e selon rate limits"""
        
        store_id = task.store_id
        
        # 1. VÃ©rifier fenÃªtre horaire
        if not self.config.is_active_hours():
            return False
        
        # 2. VÃ©rifier cooldown entre publications
        last_pub = self._last_publication.get(store_id)
        if last_pub:
            time_since_last = datetime.now() - last_pub
            if time_since_last.total_seconds() < self.config.cooldown_between_publications:
                return False
        
        # 3. VÃ©rifier token bucket
        rate_limiter = self._get_rate_limiter(store_id)
        return rate_limiter.consume(1)
    
    def _get_rate_limiter(self, store_id: str) -> TokenBucket:
        """Obtenir rate limiter pour un store (lazy creation)"""
        
        if store_id not in self._rate_limiters:
            # CrÃ©er token bucket: capacity = publications/heure, rate = tokens/seconde
            capacity = self.config.max_publications_per_hour
            refill_rate = capacity / 3600.0  # tokens par seconde
            
            self._rate_limiters[store_id] = TokenBucket(capacity, refill_rate)
            logger.debug(f"Rate limiter crÃ©Ã© pour store {store_id}: "
                        f"{capacity} tokens, {refill_rate:.4f}/sec")
        
        return self._rate_limiters[store_id]
    
    async def get_queue_stats(self) -> Dict[str, Any]:
        """Statistiques de la queue"""
        
        async with self._queue_lock:
            queue_depth = len(self._queue)
            
            # RÃ©partition par status
            status_counts = defaultdict(int)
            priority_counts = defaultdict(int)
            store_counts = defaultdict(int)
            
            for task in self._queue:
                status_counts[task.status.value] += 1
                priority_counts[task.priority] += 1
                store_counts[task.store_id] += 1
            
            return {
                **self.stats,
                'current_queue_depth': queue_depth,
                'status_distribution': dict(status_counts),
                'priority_distribution': dict(priority_counts),
                'store_distribution': dict(store_counts),
                'active_rate_limiters': len(self._rate_limiters)
            }
    
    async def get_rate_limit_stats(self) -> Dict[str, Any]:
        """Statistiques rate limiting par store"""
        
        rate_stats = {}
        for store_id, limiter in self._rate_limiters.items():
            rate_stats[store_id] = {
                **limiter.get_stats(),
                'last_publication': self._last_publication.get(store_id),
                'next_available_in_sec': limiter.time_until_available()
            }
        
        return rate_stats
    
    async def get_next_available_slot(self, store_id: str) -> datetime:
        """Calculer prochain crÃ©neau disponible pour un store"""
        
        now = datetime.now()
        
        # VÃ©rifier fenÃªtre horaire
        if not self.config.is_active_hours():
            next_slot = self.config.calculate_next_slot()
            return next_slot
        
        # VÃ©rifier cooldown
        last_pub = self._last_publication.get(store_id)
        if last_pub:
            cooldown_end = last_pub + timedelta(seconds=self.config.cooldown_between_publications)
            if cooldown_end > now:
                return cooldown_end
        
        # VÃ©rifier rate limiter
        rate_limiter = self._get_rate_limiter(store_id)
        wait_seconds = rate_limiter.time_until_available()
        
        return now + timedelta(seconds=wait_seconds)
    
    async def requeue_task(self, task: PublishTask) -> None:
        """Remettre tÃ¢che en queue (Ã©chec temporaire)"""
        
        # Reset status
        task.status = PublicationStatus.PENDING
        task.started_at = None
        
        # Baisser prioritÃ© pour Ã©viter boucle
        task.priority = min(10, task.priority + 1)
        
        await self.add_task(task)
        logger.warning(f"TÃ¢che remise en queue: {task.task_id} (nouvelle prioritÃ©: {task.priority})")


class PublishWorker:
    """Worker pour traiter les tÃ¢ches de publication"""
    
    def __init__(self, worker_id: str, queue: PublishQueue, 
                 publisher_factory: Callable[[str], Any]):
        """
        Args:
            worker_id: ID unique du worker
            queue: Queue de tÃ¢ches
            publisher_factory: Factory pour crÃ©er publishers par store_type
        """
        self.worker_id = worker_id
        self.queue = queue
        self.publisher_factory = publisher_factory
        
        self.is_running = False
        self.current_task: Optional[PublishTask] = None
        
        # Statistiques worker
        self.stats = {
            'tasks_processed': 0,
            'tasks_successful': 0,
            'tasks_failed': 0,
            'total_processing_time': 0.0,
            'start_time': None
        }
    
    async def start(self):
        """DÃ©marrer worker"""
        
        self.is_running = True
        self.stats['start_time'] = time.time()
        
        logger.info(f"ðŸ”„ Worker {self.worker_id} dÃ©marrÃ©")
        
        while self.is_running:
            try:
                # RÃ©cupÃ©rer prochaine tÃ¢che
                task = await self.queue.get_next_task()
                
                if task:
                    await self._process_task(task)
                else:
                    # Pas de tÃ¢che disponible â†’ attendre
                    await asyncio.sleep(5)
                    
            except Exception as e:
                logger.error(f"Erreur worker {self.worker_id}: {e}")
                await asyncio.sleep(10)  # Pause avant retry
    
    async def stop(self):
        """ArrÃªter worker"""
        self.is_running = False
        logger.info(f"â¹ï¸  Worker {self.worker_id} arrÃªtÃ©")
    
    async def _process_task(self, task: PublishTask):
        """Traiter une tÃ¢che de publication"""
        
        self.current_task = task
        start_time = time.time()
        
        try:
            logger.info(f"ðŸ“¤ Worker {self.worker_id} traite: {task.task_id}")
            
            # Obtenir publisher pour le type de store
            publisher = self.publisher_factory(task.store_type.value)
            if not publisher:
                raise ValueError(f"Publisher non trouvÃ© pour type: {task.store_type}")
            
            # Publication via transport robuste
            result = await publisher.publish_product(task)
            
            # Marquer succÃ¨s
            task.mark_success(result)
            self.stats['tasks_successful'] += 1
            
            duration = time.time() - start_time
            logger.info(f"âœ… TÃ¢che {task.task_id} rÃ©ussie en {duration:.2f}s")
            
        except Exception as e:
            # Marquer Ã©chec
            task.mark_failed(str(e))
            self.stats['tasks_failed'] += 1
            
            duration = time.time() - start_time
            logger.error(f"âŒ TÃ¢che {task.task_id} Ã©chouÃ©e aprÃ¨s {duration:.2f}s: {e}")
            
            # RÃ©essayer si Ã©ligible
            if task.is_retryable():
                await asyncio.sleep(60)  # Attendre avant requeue
                await self.queue.requeue_task(task)
        
        finally:
            processing_time = time.time() - start_time
            self.stats['tasks_processed'] += 1
            self.stats['total_processing_time'] += processing_time
            self.current_task = None
    
    def get_stats(self) -> Dict[str, Any]:
        """Statistiques du worker"""
        
        uptime = 0.0
        if self.stats['start_time']:
            uptime = time.time() - self.stats['start_time']
        
        avg_processing_time = 0.0
        if self.stats['tasks_processed'] > 0:
            avg_processing_time = self.stats['total_processing_time'] / self.stats['tasks_processed']
        
        return {
            **self.stats,
            'worker_id': self.worker_id,
            'is_running': self.is_running,
            'uptime_seconds': uptime,
            'avg_processing_time': avg_processing_time,
            'current_task_id': self.current_task.task_id if self.current_task else None
        }