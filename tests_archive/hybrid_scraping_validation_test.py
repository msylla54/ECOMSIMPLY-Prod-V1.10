#!/usr/bin/env python3
"""
VALIDATION FINALE SYSTÃˆME SCRAPING PRIX HYBRIDE - ECOMSIMPLY
Test complet du nouveau systÃ¨me hybride Phase 3 avec approche mock-first

COMPOSANTS Ã€ TESTER:
1. HybridScrapingService - service unifiÃ©
2. DÃ©tection Outliers Multi-MÃ©thodes  
3. Cache SystÃ¨me avec TTL et hit ratio
4. Monitoring Dashboard avec mÃ©triques temps rÃ©el
5. Integration avec Services Existants

CRITÃˆRES DE SUCCÃˆS:
- âœ… Taux succÃ¨s â‰¥95% sur scraping unifiÃ©
- âœ… Outliers dÃ©tectÃ©s avec confidence â‰¥90%
- âœ… Cache fonctionnel avec hit ratio tracking
- âœ… Dashboard metrics cohÃ©rents et temps rÃ©el
- âœ… Pas de rÃ©gression sur fonctionnalitÃ©s existantes
"""

import asyncio
import sys
import os
import time
import statistics
from typing import Dict, List, Any

# Add backend path for imports
sys.path.append('/app/backend')

from services.hybrid_scraping_service import (
    HybridScrapingService, 
    PriceOutlierDetector, 
    ScrapingCache,
    OutlierDetectionResult,
    PriceAnalysisResult
)

class HybridScrapingValidator:
    """Validateur complet du systÃ¨me de scraping hybride"""
    
    def __init__(self):
        self.hybrid_service = HybridScrapingService()
        self.outlier_detector = PriceOutlierDetector()
        self.test_results = []
        self.validation_summary = {
            "total_tests": 0,
            "passed_tests": 0,
            "failed_tests": 0,
            "critical_issues": [],
            "success_rate": 0.0
        }
    
    def log_test_result(self, test_name: str, success: bool, details: Dict = None, critical: bool = False):
        """Log rÃ©sultat de test avec dÃ©tails"""
        result = {
            "test_name": test_name,
            "success": success,
            "details": details or {},
            "critical": critical,
            "timestamp": time.time()
        }
        
        self.test_results.append(result)
        self.validation_summary["total_tests"] += 1
        
        if success:
            self.validation_summary["passed_tests"] += 1
            print(f"âœ… {test_name}")
        else:
            self.validation_summary["failed_tests"] += 1
            print(f"âŒ {test_name}")
            if critical:
                self.validation_summary["critical_issues"].append(test_name)
        
        if details:
            for key, value in details.items():
                print(f"   ğŸ“Š {key}: {value}")
    
    async def test_hybrid_scraping_service_unified(self):
        """TEST 1: HybridScrapingService - endpoint principal scrape_prices_unified()"""
        print("\nğŸ§ª TEST 1: HybridScrapingService - Service UnifiÃ©")
        print("=" * 60)
        
        test_products = [
            {"name": "iPhone 15 Pro", "category": "smartphone"},
            {"name": "Samsung Galaxy S24", "category": "smartphone"}
        ]
        
        unified_results = []
        
        for product in test_products:
            print(f"\nğŸ” Test scraping unifiÃ©: {product['name']}")
            
            try:
                start_time = time.time()
                
                # Test scraping unifiÃ©
                result = await self.hybrid_service.scrape_prices_unified(
                    product_name=product["name"],
                    category=product["category"],
                    use_cache=False  # Premier test sans cache
                )
                
                response_time = (time.time() - start_time) * 1000
                
                # Validation rÃ©sultat
                validation = {
                    "has_prices": result.found_prices > 0,
                    "sources_analyzed": result.sources_analyzed >= 3,
                    "sources_successful": result.sources_successful > 0,
                    "has_statistics": bool(result.price_statistics),
                    "outlier_analysis": result.outlier_analysis is not None,
                    "response_time_ok": response_time < 30000,  # 30 secondes max
                    "cache_status": result.cache_hit == False  # Premier appel sans cache
                }
                
                success_rate = sum(validation.values()) / len(validation)
                
                unified_results.append({
                    "product": product["name"],
                    "found_prices": result.found_prices,
                    "sources_successful": result.sources_successful,
                    "sources_analyzed": result.sources_analyzed,
                    "response_time_ms": response_time,
                    "success_rate": success_rate,
                    "validation": validation
                })
                
                print(f"   ğŸ“Š Prix trouvÃ©s: {result.found_prices}")
                print(f"   ğŸ“Š Sources rÃ©ussies: {result.sources_successful}/{result.sources_analyzed}")
                print(f"   ğŸ“Š Temps rÃ©ponse: {response_time:.0f}ms")
                print(f"   ğŸ“Š Taux succÃ¨s: {success_rate:.1%}")
                
                if result.price_statistics:
                    print(f"   ğŸ’° Prix moyen: {result.price_statistics.get('avg_price', 'N/A')}â‚¬")
                    print(f"   ğŸ’° Fourchette: {result.price_statistics.get('min_price', 'N/A')}â‚¬ - {result.price_statistics.get('max_price', 'N/A')}â‚¬")
                
            except Exception as e:
                print(f"   âŒ Exception: {str(e)}")
                unified_results.append({
                    "product": product["name"],
                    "error": str(e),
                    "success_rate": 0.0
                })
        
        # Ã‰valuation globale
        overall_success_rate = sum(r.get("success_rate", 0) for r in unified_results) / len(unified_results)
        total_prices = sum(r.get("found_prices", 0) for r in unified_results)
        
        success = overall_success_rate >= 0.95  # CritÃ¨re: â‰¥95% taux succÃ¨s
        
        self.log_test_result(
            "HybridScrapingService UnifiÃ©",
            success,
            {
                "taux_succÃ¨s_global": f"{overall_success_rate:.1%}",
                "total_prix_trouvÃ©s": total_prices,
                "produits_testÃ©s": len(test_products),
                "critÃ¨re_95%": "âœ… ATTEINT" if success else "âŒ NON ATTEINT"
            },
            critical=True
        )
        
        return success
    
    def test_outlier_detection_multi_methods(self):
        """TEST 2: DÃ©tection Outliers Multi-MÃ©thodes avec confidence â‰¥90%"""
        print("\nğŸ§ª TEST 2: DÃ©tection Outliers Multi-MÃ©thodes")
        print("=" * 60)
        
        # Test avec prix contenant outliers Ã©vidents
        test_cases = [
            {
                "name": "Prix avec outliers Ã©vidents",
                "prices": [899.99, 1199.99, 9999.99, 50.0, 1299.99],  # 9999.99 et 50.0 sont outliers
                "product": "iPhone 15 Pro",
                "expected_outliers": 2
            },
            {
                "name": "Prix normaux sans outliers",
                "prices": [899.99, 949.99, 999.99, 1049.99, 1099.99],
                "product": "Samsung Galaxy S24",
                "expected_outliers": 0
            },
            {
                "name": "Prix avec outlier contextuel",
                "prices": [25000.0, 899.99, 949.99, 999.99],  # 25000 outlier pour smartphone
                "product": "iPhone 15 Pro",
                "expected_outliers": 1
            }
        ]
        
        outlier_results = []
        
        for test_case in test_cases:
            print(f"\nğŸ” Test: {test_case['name']}")
            print(f"   Prix testÃ©s: {test_case['prices']}")
            
            try:
                # Test dÃ©tection combinÃ©e
                result = self.outlier_detector.detect_outliers_combined(
                    prices=test_case["prices"],
                    product_name=test_case["product"]
                )
                
                # Validation
                validation = {
                    "outliers_detected": len(result.outliers_detected),
                    "clean_prices_count": len(result.clean_prices),
                    "confidence_score": result.confidence_score,
                    "method_used": result.method_used,
                    "expected_outliers": test_case["expected_outliers"]
                }
                
                confidence_ok = result.confidence_score >= 0.90
                outliers_reasonable = abs(len(result.outliers_detected) - test_case["expected_outliers"]) <= 1
                
                outlier_results.append({
                    "test_case": test_case["name"],
                    "confidence_score": result.confidence_score,
                    "outliers_detected": len(result.outliers_detected),
                    "confidence_ok": confidence_ok,
                    "outliers_reasonable": outliers_reasonable,
                    "validation": validation
                })
                
                print(f"   ğŸ“Š Outliers dÃ©tectÃ©s: {len(result.outliers_detected)}")
                print(f"   ğŸ“Š Prix propres: {len(result.clean_prices)}")
                print(f"   ğŸ“Š Confidence: {result.confidence_score:.1%}")
                print(f"   ğŸ“Š MÃ©thode: {result.method_used}")
                
                if result.outliers_detected:
                    for outlier in result.outliers_detected:
                        print(f"      ğŸš« Outlier: {outlier['price']}â‚¬ ({outlier.get('reason', 'N/A')})")
                
            except Exception as e:
                print(f"   âŒ Exception: {str(e)}")
                outlier_results.append({
                    "test_case": test_case["name"],
                    "error": str(e),
                    "confidence_ok": False,
                    "outliers_reasonable": False
                })
        
        # Ã‰valuation globale
        confidence_tests_passed = sum(1 for r in outlier_results if r.get("confidence_ok", False))
        outlier_tests_passed = sum(1 for r in outlier_results if r.get("outliers_reasonable", False))
        
        success = (confidence_tests_passed >= len(test_cases) * 0.8 and 
                  outlier_tests_passed >= len(test_cases) * 0.8)
        
        avg_confidence = sum(r.get("confidence_score", 0) for r in outlier_results) / len(outlier_results)
        
        self.log_test_result(
            "DÃ©tection Outliers Multi-MÃ©thodes",
            success,
            {
                "confidence_moyenne": f"{avg_confidence:.1%}",
                "tests_confidence_â‰¥90%": f"{confidence_tests_passed}/{len(test_cases)}",
                "tests_outliers_corrects": f"{outlier_tests_passed}/{len(test_cases)}",
                "critÃ¨re_90%": "âœ… ATTEINT" if avg_confidence >= 0.90 else "âŒ NON ATTEINT"
            },
            critical=True
        )
        
        return success
    
    async def test_cache_system_functionality(self):
        """TEST 3: Cache SystÃ¨me avec TTL et hit ratio tracking"""
        print("\nğŸ§ª TEST 3: Cache SystÃ¨me avec TTL et Hit Ratio")
        print("=" * 60)
        
        cache_results = []
        
        # Test 1: Cache miss puis cache hit
        print("\nğŸ” Test Cache Miss â†’ Hit")
        
        try:
            # Premier appel (cache miss)
            result1 = await self.hybrid_service.scrape_prices_unified(
                "iPhone 15 Pro", use_cache=True
            )
            
            # DeuxiÃ¨me appel immÃ©diat (cache hit)
            result2 = await self.hybrid_service.scrape_prices_unified(
                "iPhone 15 Pro", use_cache=True
            )
            
            cache_miss_ok = result1.cache_hit == False
            cache_hit_ok = result2.cache_hit == True
            
            print(f"   ğŸ“Š Premier appel (miss): {result1.cache_hit}")
            print(f"   ğŸ“Š DeuxiÃ¨me appel (hit): {result2.cache_hit}")
            
            cache_results.append({
                "test": "cache_miss_hit",
                "cache_miss_ok": cache_miss_ok,
                "cache_hit_ok": cache_hit_ok,
                "success": cache_miss_ok and cache_hit_ok
            })
            
        except Exception as e:
            print(f"   âŒ Exception cache miss/hit: {str(e)}")
            cache_results.append({
                "test": "cache_miss_hit",
                "error": str(e),
                "success": False
            })
        
        # Test 2: Stats du cache
        print("\nğŸ” Test Cache Stats")
        
        try:
            cache_stats = self.hybrid_service.cache.get_cache_stats()
            
            stats_validation = {
                "has_total_requests": cache_stats.get("total_requests", 0) > 0,
                "has_cache_hits": cache_stats.get("cache_hits", 0) > 0,
                "has_hit_ratio": "hit_ratio" in cache_stats,
                "has_cached_entries": cache_stats.get("cached_entries", 0) > 0
            }
            
            hit_ratio = cache_stats.get("hit_ratio", 0)
            
            print(f"   ğŸ“Š Total requÃªtes: {cache_stats.get('total_requests', 0)}")
            print(f"   ğŸ“Š Cache hits: {cache_stats.get('cache_hits', 0)}")
            print(f"   ğŸ“Š Cache misses: {cache_stats.get('cache_misses', 0)}")
            print(f"   ğŸ“Š Hit ratio: {hit_ratio:.1%}")
            print(f"   ğŸ“Š EntrÃ©es cachÃ©es: {cache_stats.get('cached_entries', 0)}")
            
            cache_results.append({
                "test": "cache_stats",
                "hit_ratio": hit_ratio,
                "stats_complete": all(stats_validation.values()),
                "success": all(stats_validation.values())
            })
            
        except Exception as e:
            print(f"   âŒ Exception cache stats: {str(e)}")
            cache_results.append({
                "test": "cache_stats",
                "error": str(e),
                "success": False
            })
        
        # Test 3: TTL expiration (simulation)
        print("\nğŸ” Test TTL Expiration (simulation)")
        
        try:
            # Forcer expiration en modifiant TTL
            original_ttl = self.hybrid_service.cache.ttl_seconds
            self.hybrid_service.cache.ttl_seconds = 1  # 1 seconde
            
            # Premier appel
            await self.hybrid_service.scrape_prices_unified("Test TTL Product", use_cache=True)
            
            # Attendre expiration
            await asyncio.sleep(2)
            
            # DeuxiÃ¨me appel (devrait Ãªtre miss Ã  cause de l'expiration)
            result_after_ttl = await self.hybrid_service.scrape_prices_unified("Test TTL Product", use_cache=True)
            
            # Restaurer TTL original
            self.hybrid_service.cache.ttl_seconds = original_ttl
            
            ttl_working = result_after_ttl.cache_hit == False
            
            print(f"   ğŸ“Š Cache aprÃ¨s TTL expiration: {result_after_ttl.cache_hit} (devrait Ãªtre False)")
            
            cache_results.append({
                "test": "ttl_expiration",
                "ttl_working": ttl_working,
                "success": ttl_working
            })
            
        except Exception as e:
            print(f"   âŒ Exception TTL test: {str(e)}")
            cache_results.append({
                "test": "ttl_expiration",
                "error": str(e),
                "success": False
            })
        
        # Ã‰valuation globale cache
        successful_cache_tests = sum(1 for r in cache_results if r.get("success", False))
        total_cache_tests = len(cache_results)
        
        success = successful_cache_tests >= total_cache_tests * 0.8
        
        self.log_test_result(
            "Cache SystÃ¨me Fonctionnel",
            success,
            {
                "tests_rÃ©ussis": f"{successful_cache_tests}/{total_cache_tests}",
                "hit_ratio_tracking": "âœ… FONCTIONNEL" if any(r.get("hit_ratio") is not None for r in cache_results) else "âŒ DÃ‰FAILLANT",
                "ttl_expiration": "âœ… FONCTIONNEL" if any(r.get("ttl_working") for r in cache_results) else "âŒ DÃ‰FAILLANT"
            },
            critical=True
        )
        
        return success
    
    def test_monitoring_dashboard_data(self):
        """TEST 4: Monitoring Dashboard avec mÃ©triques temps rÃ©el"""
        print("\nğŸ§ª TEST 4: Monitoring Dashboard - MÃ©triques Temps RÃ©el")
        print("=" * 60)
        
        dashboard_results = []
        
        # Test 1: get_monitoring_dashboard_data()
        print("\nğŸ” Test Dashboard Data")
        
        try:
            dashboard_data = self.hybrid_service.get_monitoring_dashboard_data()
            
            # Validation structure dashboard
            required_sections = ["overview", "sources_performance", "cache_performance", "proxy_health"]
            sections_present = all(section in dashboard_data for section in required_sections)
            
            # Validation overview
            overview = dashboard_data.get("overview", {})
            overview_fields = ["total_requests", "success_rate", "avg_response_time_ms", "total_prices_found"]
            overview_complete = all(field in overview for field in overview_fields)
            
            # Validation sources performance
            sources_perf = dashboard_data.get("sources_performance", {})
            sources_count = len(sources_perf)
            
            print(f"   ğŸ“Š Sections prÃ©sentes: {sections_present}")
            print(f"   ğŸ“Š Overview complet: {overview_complete}")
            print(f"   ğŸ“Š Sources trackÃ©es: {sources_count}")
            print(f"   ğŸ“Š Total requÃªtes: {overview.get('total_requests', 0)}")
            print(f"   ğŸ“Š Taux succÃ¨s: {overview.get('success_rate', 0):.1%}")
            print(f"   ğŸ“Š Temps rÃ©ponse moyen: {overview.get('avg_response_time_ms', 0):.0f}ms")
            
            dashboard_results.append({
                "test": "dashboard_data",
                "sections_present": sections_present,
                "overview_complete": overview_complete,
                "sources_count": sources_count,
                "success": sections_present and overview_complete and sources_count > 0
            })
            
        except Exception as e:
            print(f"   âŒ Exception dashboard data: {str(e)}")
            dashboard_results.append({
                "test": "dashboard_data",
                "error": str(e),
                "success": False
            })
        
        # Test 2: get_system_health_status()
        print("\nğŸ” Test System Health Status")
        
        try:
            health_status = self.hybrid_service.get_system_health_status()
            
            # Validation structure health
            required_health_fields = ["system_status", "components", "metrics", "recommendations"]
            health_complete = all(field in health_status for field in required_health_fields)
            
            # Validation components
            components = health_status.get("components", {})
            expected_components = ["scraping_service", "proxy_provider", "cache_system", "outlier_detection"]
            components_complete = all(comp in components for comp in expected_components)
            
            system_status = health_status.get("system_status", "unknown")
            recommendations = health_status.get("recommendations", [])
            
            print(f"   ğŸ“Š Structure santÃ© complÃ¨te: {health_complete}")
            print(f"   ğŸ“Š Composants complets: {components_complete}")
            print(f"   ğŸ“Š Status systÃ¨me: {system_status}")
            print(f"   ğŸ“Š Recommandations: {len(recommendations)}")
            
            for comp, status in components.items():
                print(f"      ğŸ”§ {comp}: {status}")
            
            dashboard_results.append({
                "test": "health_status",
                "health_complete": health_complete,
                "components_complete": components_complete,
                "system_status": system_status,
                "success": health_complete and components_complete
            })
            
        except Exception as e:
            print(f"   âŒ Exception health status: {str(e)}")
            dashboard_results.append({
                "test": "health_status",
                "error": str(e),
                "success": False
            })
        
        # Ã‰valuation globale dashboard
        successful_dashboard_tests = sum(1 for r in dashboard_results if r.get("success", False))
        total_dashboard_tests = len(dashboard_results)
        
        success = successful_dashboard_tests == total_dashboard_tests
        
        self.log_test_result(
            "Monitoring Dashboard CohÃ©rent",
            success,
            {
                "tests_rÃ©ussis": f"{successful_dashboard_tests}/{total_dashboard_tests}",
                "dashboard_data": "âœ… FONCTIONNEL" if any(r.get("test") == "dashboard_data" and r.get("success") for r in dashboard_results) else "âŒ DÃ‰FAILLANT",
                "health_status": "âœ… FONCTIONNEL" if any(r.get("test") == "health_status" and r.get("success") for r in dashboard_results) else "âŒ DÃ‰FAILLANT"
            },
            critical=True
        )
        
        return success
    
    def test_integration_existing_services(self):
        """TEST 5: Integration avec Services Existants"""
        print("\nğŸ§ª TEST 5: Integration avec Services Existants")
        print("=" * 60)
        
        integration_results = []
        
        # Test 1: CompatibilitÃ© enhanced_scraping_service
        print("\nğŸ” Test CompatibilitÃ© Enhanced Scraping Service")
        
        try:
            # VÃ©rifier que le service enhanced est bien intÃ©grÃ©
            enhanced_service = self.hybrid_service.enhanced_service
            has_enhanced = enhanced_service is not None
            
            # VÃ©rifier mÃ©thodes disponibles
            has_scrape_method = hasattr(enhanced_service, 'scrape_competitor_prices_enhanced')
            
            print(f"   ğŸ“Š Enhanced service intÃ©grÃ©: {has_enhanced}")
            print(f"   ğŸ“Š MÃ©thode scraping disponible: {has_scrape_method}")
            
            integration_results.append({
                "test": "enhanced_service_integration",
                "has_enhanced": has_enhanced,
                "has_scrape_method": has_scrape_method,
                "success": has_enhanced and has_scrape_method
            })
            
        except Exception as e:
            print(f"   âŒ Exception enhanced service: {str(e)}")
            integration_results.append({
                "test": "enhanced_service_integration",
                "error": str(e),
                "success": False
            })
        
        # Test 2: Proxy provider integration
        print("\nğŸ” Test Proxy Provider Integration")
        
        try:
            # VÃ©rifier proxy provider
            proxy_provider = self.hybrid_service.proxy_provider
            has_proxy = proxy_provider is not None
            
            # VÃ©rifier stats proxy
            proxy_stats = proxy_provider.get_provider_stats() if has_proxy else {}
            has_stats = bool(proxy_stats)
            
            print(f"   ğŸ“Š Proxy provider intÃ©grÃ©: {has_proxy}")
            print(f"   ğŸ“Š Stats proxy disponibles: {has_stats}")
            
            if has_stats:
                print(f"      ğŸ”§ Proxies totaux: {proxy_stats.get('total_proxies', 0)}")
                print(f"      ğŸ”§ Proxies sains: {proxy_stats.get('healthy_proxies', 0)}")
            
            integration_results.append({
                "test": "proxy_provider_integration",
                "has_proxy": has_proxy,
                "has_stats": has_stats,
                "success": has_proxy and has_stats
            })
            
        except Exception as e:
            print(f"   âŒ Exception proxy provider: {str(e)}")
            integration_results.append({
                "test": "proxy_provider_integration",
                "error": str(e),
                "success": False
            })
        
        # Test 3: Configuration environnement MOCK_MODE
        print("\nğŸ” Test Configuration Environnement")
        
        try:
            # VÃ©rifier variables d'environnement
            mock_mode = os.environ.get('MOCK_MODE', 'true').lower() == 'true'
            scraping_source_set = os.environ.get('SCRAPING_SOURCE_SET', 'default')
            
            # VÃ©rifier sources configurÃ©es
            sources = self.hybrid_service.price_sources
            sources_count = len(sources)
            
            print(f"   ğŸ“Š MOCK_MODE: {mock_mode}")
            print(f"   ğŸ“Š SCRAPING_SOURCE_SET: {scraping_source_set}")
            print(f"   ğŸ“Š Sources configurÃ©es: {sources_count}")
            
            for source in sources:
                print(f"      ğŸ”§ {source['name']}: prioritÃ© {source['priority']}, poids {source['weight']}")
            
            integration_results.append({
                "test": "environment_configuration",
                "mock_mode": mock_mode,
                "sources_count": sources_count,
                "success": sources_count >= 3  # Au moins 3 sources
            })
            
        except Exception as e:
            print(f"   âŒ Exception configuration: {str(e)}")
            integration_results.append({
                "test": "environment_configuration",
                "error": str(e),
                "success": False
            })
        
        # Ã‰valuation globale integration
        successful_integration_tests = sum(1 for r in integration_results if r.get("success", False))
        total_integration_tests = len(integration_results)
        
        success = successful_integration_tests >= total_integration_tests * 0.8
        
        self.log_test_result(
            "Integration Services Existants",
            success,
            {
                "tests_rÃ©ussis": f"{successful_integration_tests}/{total_integration_tests}",
                "enhanced_service": "âœ… INTÃ‰GRÃ‰" if any(r.get("test") == "enhanced_service_integration" and r.get("success") for r in integration_results) else "âŒ PROBLÃˆME",
                "proxy_provider": "âœ… INTÃ‰GRÃ‰" if any(r.get("test") == "proxy_provider_integration" and r.get("success") for r in integration_results) else "âŒ PROBLÃˆME",
                "configuration": "âœ… CORRECTE" if any(r.get("test") == "environment_configuration" and r.get("success") for r in integration_results) else "âŒ PROBLÃˆME"
            },
            critical=False
        )
        
        return success
    
    async def run_comprehensive_validation(self):
        """ExÃ©cute la validation complÃ¨te du systÃ¨me hybride"""
        print("ğŸš€ VALIDATION FINALE SYSTÃˆME SCRAPING PRIX HYBRIDE - ECOMSIMPLY")
        print("=" * 80)
        print("Objectif: Valider que le systÃ¨me hybride atteint 95% taux succÃ¨s")
        print("Mode: Mock-first avec composants critiques fonctionnels")
        print("=" * 80)
        
        start_time = time.time()
        
        try:
            # ExÃ©cution des 5 tests critiques
            print("\nğŸ¯ DÃ‰MARRAGE VALIDATION HYBRIDE...")
            
            test1_result = await self.test_hybrid_scraping_service_unified()
            await asyncio.sleep(1)
            
            test2_result = self.test_outlier_detection_multi_methods()
            await asyncio.sleep(1)
            
            test3_result = await self.test_cache_system_functionality()
            await asyncio.sleep(1)
            
            test4_result = self.test_monitoring_dashboard_data()
            await asyncio.sleep(1)
            
            test5_result = self.test_integration_existing_services()
            
            # Calcul final
            total_time = time.time() - start_time
            self.validation_summary["success_rate"] = (
                self.validation_summary["passed_tests"] / 
                self.validation_summary["total_tests"] * 100
            ) if self.validation_summary["total_tests"] > 0 else 0
            
            # RÃ©sumÃ© final
            print("\n" + "=" * 80)
            print("ğŸ RÃ‰SUMÃ‰ FINAL - VALIDATION SYSTÃˆME HYBRIDE")
            print("=" * 80)
            
            print(f"ğŸ“Š Total Tests: {self.validation_summary['total_tests']}")
            print(f"âœ… RÃ©ussis: {self.validation_summary['passed_tests']}")
            print(f"âŒ Ã‰chouÃ©s: {self.validation_summary['failed_tests']}")
            print(f"ğŸ“ˆ Taux de RÃ©ussite Global: {self.validation_summary['success_rate']:.1f}%")
            print(f"â±ï¸ Temps Total: {total_time:.1f}s")
            
            print(f"\nğŸ¯ STATUT DES COMPOSANTS CRITIQUES:")
            print(f"   1. HybridScrapingService UnifiÃ©: {'âœ… RÃ‰USSI' if test1_result else 'âŒ Ã‰CHOUÃ‰'}")
            print(f"   2. DÃ©tection Outliers Multi-MÃ©thodes: {'âœ… RÃ‰USSI' if test2_result else 'âŒ Ã‰CHOUÃ‰'}")
            print(f"   3. Cache SystÃ¨me Fonctionnel: {'âœ… RÃ‰USSI' if test3_result else 'âŒ Ã‰CHOUÃ‰'}")
            print(f"   4. Monitoring Dashboard CohÃ©rent: {'âœ… RÃ‰USSI' if test4_result else 'âŒ Ã‰CHOUÃ‰'}")
            print(f"   5. Integration Services Existants: {'âœ… RÃ‰USSI' if test5_result else 'âŒ Ã‰CHOUÃ‰'}")
            
            # CritÃ¨res de succÃ¨s Phase 3
            success_criteria = {
                "taux_succÃ¨s_â‰¥95%": test1_result,
                "outliers_confidence_â‰¥90%": test2_result,
                "cache_fonctionnel": test3_result,
                "dashboard_cohÃ©rent": test4_result,
                "pas_rÃ©gression": test5_result
            }
            
            print(f"\nğŸ“‹ CRITÃˆRES DE SUCCÃˆS PHASE 3:")
            for criterion, met in success_criteria.items():
                status_icon = "âœ…" if met else "âŒ"
                print(f"   {status_icon} {criterion}")
            
            # Ã‰valuation finale
            critical_success = test1_result and test2_result and test3_result  # 3 composants critiques
            overall_success = all(success_criteria.values())
            
            if overall_success:
                print(f"\nğŸ‰ SUCCÃˆS COMPLET: Le systÃ¨me hybride est PRODUCTION-READY!")
                print("   âœ… Taux succÃ¨s â‰¥95% sur scraping unifiÃ©")
                print("   âœ… Outliers dÃ©tectÃ©s avec confidence â‰¥90%")
                print("   âœ… Cache fonctionnel avec hit ratio tracking")
                print("   âœ… Dashboard metrics cohÃ©rents et temps rÃ©el")
                print("   âœ… Pas de rÃ©gression sur fonctionnalitÃ©s existantes")
                print("\nğŸš€ Le systÃ¨me hybride Phase 3 peut Ãªtre dÃ©ployÃ© en production!")
                
            elif critical_success:
                print(f"\nâš¡ SUCCÃˆS PARTIEL: Les composants critiques fonctionnent!")
                print("   âœ… Scraping unifiÃ© opÃ©rationnel")
                print("   âœ… DÃ©tection outliers fonctionnelle")
                print("   âœ… Cache systÃ¨me opÃ©rationnel")
                if not test4_result:
                    print("   âš ï¸ Dashboard monitoring nÃ©cessite des ajustements")
                if not test5_result:
                    print("   âš ï¸ IntÃ©gration services nÃ©cessite vÃ©rification")
                print("\nğŸ”§ Corrections mineures recommandÃ©es avant production")
                
            else:
                print(f"\nâŒ Ã‰CHEC CRITIQUE: Le systÃ¨me hybride prÃ©sente des problÃ¨mes majeurs")
                if not test1_result:
                    print("   âŒ Scraping unifiÃ© non fonctionnel")
                if not test2_result:
                    print("   âŒ DÃ©tection outliers dÃ©faillante")
                if not test3_result:
                    print("   âŒ Cache systÃ¨me dÃ©faillant")
                print("   ğŸ”§ Correction immÃ©diate requise avant dÃ©ploiement")
            
            # Issues critiques
            if self.validation_summary["critical_issues"]:
                print(f"\nğŸš¨ ISSUES CRITIQUES Ã€ RÃ‰SOUDRE:")
                for issue in self.validation_summary["critical_issues"]:
                    print(f"   âŒ {issue}")
            
            return critical_success
            
        except Exception as e:
            print(f"\nâŒ ERREUR CRITIQUE VALIDATION: {str(e)}")
            return False

async def main():
    """ExÃ©cution principale de la validation"""
    validator = HybridScrapingValidator()
    success = await validator.run_comprehensive_validation()
    
    # Code de sortie appropriÃ©
    exit(0 if success else 1)

if __name__ == "__main__":
    asyncio.run(main())